%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************
\chapter{Derivations} 

\section{Model}
\subsection{Posterior Distribution}
\label{A:derivation-posterior-over-weights}
The prior distribution over the weights $p(\mathbf{w})$ and the model likelihood $p(\mathbf{y} | \mathbf{X}, \mathbf{w}, \sigma_{n}^{2})$ are 
\begin{equation}
    p\left(\mathbf{w}\right)= \mathcal{N}\left(\mathbf{w} | \mathbf{0}, \frac{\sigma_{0}^{2}}{m}\mathbf{I}\right)
\end{equation}
\begin{equation}
    p\left(\mathbf{y} | \mathbf{X}, \mathbf{w}, \sigma_{n}^{2}\right)=\prod_{n=1}^{N} \mathcal{N}\left(y_{n} | \mathbf{w}^{\mathrm{T}} \varphi\left(\mathbf{x}_{n}\right), \sigma_{n}^{2}\right)
\end{equation}
The posterior distribution is proportional to the product of the prior and the likelihood
\begin{equation}
    p(\mathbf{w} | \mathbf{y}) \propto \prod_{n=1}^{N} \mathcal{N}\left(y_{n} | \mathbf{w}^{\mathrm{T}} \varphi\left(\mathbf{x}_{n}\right), \sigma_{n}^{2}\right) \mathcal{N}\left(\mathbf{w} | \mathbf{0}, \frac{\sigma_{0}^{2}}{m}\mathbf{I}\right).
\end{equation}
Focusing on the exponential term $E$
\begin{equation}
    \begin{aligned}  E 
    &=-\frac{1}{2\sigma_{n}^{2}} \sum_{n=1}^{N}\left\{y_{n}-\mathbf{w}^{\top} \varphi\left(\mathbf{x}_{n}\right)\right\}^{2}-\frac{m}{2\sigma_{0}^2}\mathbf{w}^{\top} \mathbf{w} \\ 
    &=-\frac{1}{2\sigma_{n}^{2}} \sum_{n=1}^{N}\left\{y_{n}^{2}-2 y_{n} \mathbf{w}^{\top} \varphi\left(\mathbf{x}_{n}\right)+\mathbf{w}^{\top} \varphi\left(\mathbf{x}_{n}\right) \varphi\left(\mathbf{x}_{n}\right)^{\top} \mathbf{w}\right\}-\frac{m}{2\sigma_{0}^2}\mathbf{w}^{\top} \mathbf{w} \\ 
    &= -\frac{1}{2} \mathbf{w}^{\top}\left[\sum_{n=1}^{N} \frac{1}{\sigma_{n}^{2}} \varphi\left(\mathbf{x}_{n}\right) \varphi\left(\mathbf{x}_{n}\right)^{\top} + \frac{m}{\sigma_{0}^2}\mathbf{I}\right] \mathbf{w} -\frac{1}{2}\left[-\sum_{n=1}^{N}  \frac{2}{\sigma_{n}^{2}} y_{n} \varphi\left(\mathbf{x}_{n}\right)^{\top}\right] \mathbf{w} + \mathbf{C}
    \end{aligned}
    \label{A-posterior-complete-square}
\end{equation}
Through comparison of the quadratic term of the above equation with that of the standard Gaussian distribution, it can be seen that posterior precision matrix $\mathbf{A}$ is
\begin{equation}
    \mathbf{A}=\frac{1}{\sigma_{n}^{2}} \Phi \Phi^{\top} + \frac{m}{\sigma_{0}^2}\mathbf{I}.
\end{equation}
Now comparing the linear term from Equation \ref{A-posterior-complete-square} with that of a standard Gaussian distribution gives
\begin{equation}
    - \boldgreek{\mu}^{\top} \mathbf{A}=-\sum_{n=1}^{N}  \frac{1}{\sigma_{n}^{2}} y_{n} \varphi\left(\mathbf{x}_{n}\right)^{\top}.
\end{equation}
Transposing both sides (noting that $\mathbf{A}$ is symmetrical) and multiplying through by the posterior precision gives the posterior mean $\boldgreek{\mu}_{p}$ as
\begin{equation}
    \boldgreek{\mu}_{p}=\frac{1}{\sigma_{n}^{2}}\mathbf{A}^{-1} \Phi \mathbf{y}
\end{equation}

\subsection{Predictive Distribution}
\label{A:derivation-predictive-distribution}
Consider a marginal Gaussian distribution for $\textbf{x}$ and a conditional Gaussian distribution for $\textbf{y}$ given $\textbf{x}$
\begin{equation}
    p(\mathbf{x})=\mathcal{N}\left(\mathbf{x} | \boldgreek{\mu}, \boldgreek{\Lambda}\right)
\end{equation}
\begin{equation}
    p(\mathbf{y} | \mathbf{x})=\mathcal{N}\left(\mathbf{y} | \mathbf{C} \mathbf{x}+\mathbf{b}, \boldgreek{\Gamma}\right)
\end{equation}
where $\mathbf{C}$, $\mathbf{b}$ and $\boldgreek{\mu}$ are the mean parameters and $\boldgreek{\Lambda}$ and $\boldgreek{\Gamma}$ are the covariance matrices. The marginal distribution of $\mathbf{y}$ is then defined as
\begin{equation}
    p(\mathbf{y})=\int p(\mathbf{y}|\mathbf{x})p(\mathbf{x})d\mathbf{x}.
\end{equation}
The standard result for a marginal Gaussian distribution under these conditions is \cite{bishop2006pattern}
\begin{equation}
    p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} | \mathbf{C} \boldgreek{\mu}+\mathbf{b}, \boldgreek{\Gamma}+\mathbf{C} \boldgreek{\Lambda} \mathbf{C}^{\mathrm{T}}\right).
    \label{Eq:A-marginal-Gaussian-distribution}
\end{equation}
Reproducing Equations \ref{Eq:Model-posterior-over-weights} and \ref{Eq:Model-one-value-likelihood} (for a new input $\mathbf{x}_{*}$) here and noting that $f(\mathbf{x}_{*},\mathbf{w})=\mathbf{w}^{\mathrm{T}}\varphi\left(\mathbf{x}_{*}\right)=\varphi\left(\mathbf{x}_{*}\right)^{\mathrm{T}}\mathbf{w}$ gives
\begin{equation}
    p\left(\mathbf{w} | \mathbf{y}, \mathbf{X}, \sigma_{0}^{2}, \sigma_{n}^{2}\right)=\mathcal{N}\left(\mathbf{w} | \boldgreek{\mu}_{p}, \mathbf{A}^{-1}\right)
\end{equation}
\begin{equation}
    p\left(y_{*} | \mathbf{x}_{*}, \mathbf{w}, \sigma_{n}^{2}\right)=\mathcal{N}\left(y_{*} |  \varphi\left(\mathbf{x}_{*}\right)^{\mathrm{T}}\mathbf{w}, \sigma_{n}^{2}\right).
\end{equation}
Comparing these to the general result given in Equation \ref{Eq:A-marginal-Gaussian-distribution} and noting that $\varphi(\mathbf{x}_{*})^{\top} \mathbf{A}^{-1}  \varphi(\mathbf{x}_{*})=\varphi(\mathbf{x}_{*}) \mathbf{A}^{-1} \varphi(\mathbf{x}_{*})^{\top}$ gives the predictive mean $\mu_{y_{*}}$ and covariance $\sigma_{y_{*}}$
\begin{equation}
    \mu_{y_{*}}=\frac{1}{\sigma_{n}^{2}}\varphi(\mathbf{x}_{*})^{\top}\mathbf{A}^{-1}\Phi\mathbf{y}
\end{equation}
\begin{equation}
    \sigma_{y_{*}}^{2}=\sigma_{n}^{2}+\varphi(\mathbf{x}_{*})^{\top} \mathbf{A}^{-1} \varphi(\mathbf{x}_{*}).
\end{equation}
