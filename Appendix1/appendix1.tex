%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************
\chapter{Derivations} 

\section{Model}
\subsection{Posterior Distribution}
\label{A:derivation-posterior-over-weights}
The prior distribution over the weights $p(\mathbf{w})$ and the model likelihood $p(\boldgreek{\Delta} | \mathbf{X}, \mathbf{w}, \sigma_{n}^{2})$ are 
\begin{equation}
    p\left(\mathbf{w}\right)= \mathcal{N}\left(\mathbf{w} | \mathbf{0}, \frac{\sigma_{0}^{2}}{m}\mathbf{I}\right)
\end{equation}
\begin{equation}
    p\left(\boldgreek{\Delta} | \mathbf{X}, \mathbf{w}, \sigma_{n}^{2}\right)=\prod_{i=1}^{N} \mathcal{N}\left(\Delta_{i} | \mathbf{w}^{\mathrm{T}} \varphi\left(\mathbf{x}_{i}\right), \sigma_{n}^{2}\right).
\end{equation}
Here, $\mathbf{x}$ is used instead of $\mathbf{\tilde{x}}$ for notational convenience. The posterior distribution is proportional to the product of the prior and the likelihood
\begin{equation}
    p(\mathbf{w} | \boldgreek{\Delta}) \propto \prod_{i=1}^{N} \mathcal{N}\left(\Delta_{i} | \mathbf{w}^{\mathrm{T}} \varphi\left(\mathbf{x}_{i}\right), \sigma_{n}^{2}\right) \mathcal{N}\left(\mathbf{w} | \mathbf{0}, \frac{\sigma_{0}^{2}}{m}\mathbf{I}\right).
\end{equation}
Focusing on the exponential term $E$
\begin{equation}
    \begin{aligned}  E 
    &=-\frac{1}{2\sigma_{n}^{2}} \sum_{i=1}^{N}\left\{\Delta_{i}-\mathbf{w}^{\top} \varphi\left(\mathbf{x}_{i}\right)\right\}^{2}-\frac{m}{2\sigma_{0}^2}\mathbf{w}^{\top} \mathbf{w} \\ 
    &=-\frac{1}{2\sigma_{n}^{2}} \sum_{i=1}^{N}\left\{\Delta_{i}^{2}-2 \Delta_{i} \mathbf{w}^{\top} \varphi\left(\mathbf{x}_{i}\right)+\mathbf{w}^{\top} \varphi\left(\mathbf{x}_{i}\right) \varphi\left(\mathbf{x}_{i}\right)^{\top} \mathbf{w}\right\}-\frac{m}{2\sigma_{0}^2}\mathbf{w}^{\top} \mathbf{w} \\ 
    &= -\frac{1}{2} \mathbf{w}^{\top}\left[\sum_{i=1}^{N} \frac{1}{\sigma_{n}^{2}} \varphi\left(\mathbf{x}_{i}\right) \varphi\left(\mathbf{x}_{i}\right)^{\top} + \frac{m}{\sigma_{0}^2}\mathbf{I}\right] \mathbf{w} -\frac{1}{2}\left[-\sum_{i=1}^{N}  \frac{2}{\sigma_{n}^{2}} \Delta_{i} \varphi\left(\mathbf{x}_{i}\right)^{\top}\right] \mathbf{w} + \mathbf{C}
    \end{aligned}
    \label{A-posterior-complete-square}
\end{equation}
Through comparison of the quadratic term of the above equation with that of the standard Gaussian distribution, it can be seen that posterior precision matrix $\mathbf{A}$ is
\begin{equation}
    \mathbf{A}=\frac{1}{\sigma_{n}^{2}} \Phi \Phi^{\top} + \frac{m}{\sigma_{0}^2}\mathbf{I}.
\end{equation}
Now comparing the linear term from Equation \ref{A-posterior-complete-square} with that of a standard Gaussian distribution gives
\begin{equation}
    - \boldgreek{\mu}^{\top} \mathbf{A}=-\sum_{i=1}^{N}  \frac{1}{\sigma_{n}^{2}} \Delta_{i} \varphi\left(\mathbf{x}_{i}\right)^{\top}.
\end{equation}
Transposing both sides (noting that $\mathbf{A}$ is symmetrical) and multiplying through by the posterior precision gives the posterior mean $\boldgreek{\mu}_{\mathbf{w}}$ as
\begin{equation}
    \boldgreek{\mu}_{\mathbf{w}}=\frac{1}{\sigma_{n}^{2}}\mathbf{A}^{-1} \Phi \boldgreek{\Delta}
\end{equation}

\subsection{Predictive Distribution}
\label{A:derivation-predictive-distribution}
Consider a marginal Gaussian distribution for $\textbf{x}$ and a conditional Gaussian distribution for $\textbf{y}$ given $\textbf{x}$
\begin{equation}
    p(\mathbf{x})=\mathcal{N}\left(\mathbf{x} | \boldgreek{\mu}, \boldgreek{\Lambda}\right)
\end{equation}
\begin{equation}
    p(\boldgreek{\Delta} | \mathbf{x})=\mathcal{N}\left(\boldgreek{\Delta} | \mathbf{C} \mathbf{x}+\mathbf{b}, \boldgreek{\Gamma}\right)
\end{equation}
where $\mathbf{C}$, $\mathbf{b}$ and $\boldgreek{\mu}$ are the mean parameters and $\boldgreek{\Lambda}$ and $\boldgreek{\Gamma}$ are the covariance matrices. The marginal distribution of $\boldgreek{\Delta}$ is then defined as
\begin{equation}
    p(\boldgreek{\Delta})=\int p(\boldgreek{\Delta}|\mathbf{x})p(\mathbf{x})d\mathbf{x}.
\end{equation}
The standard result for a marginal Gaussian distribution under these conditions is \cite{bishop2006pattern}
\begin{equation}
    p(\boldgreek{\Delta})=\mathcal{N}\left(\boldgreek{\Delta} | \mathbf{C} \boldgreek{\mu}+\mathbf{b}, \boldgreek{\Gamma}+\mathbf{C} \boldgreek{\Lambda} \mathbf{C}^{\mathrm{T}}\right).
    \label{Eq:A-marginal-Gaussian-distribution}
\end{equation}
Reproducing Equations \ref{Eq:Model-posterior-over-weights} and \ref{Eq:Model-one-value-likelihood} (for a new input $\mathbf{x}_{*}$) here and noting that $f(\mathbf{x}_{*},\mathbf{w})=\mathbf{w}^{\mathrm{T}}\varphi\left(\mathbf{x}_{*}\right)=\varphi\left(\mathbf{x}_{*}\right)^{\mathrm{T}}\mathbf{w}$ gives
\begin{equation}
    p\left(\mathbf{w} | \boldgreek{\Delta}, \mathbf{X}, \sigma_{0}^{2}, \sigma_{n}^{2}\right)=\mathcal{N}\left(\mathbf{w} | \boldgreek{\mu}_{p}, \mathbf{A}^{-1}\right)
\end{equation}
\begin{equation}
    p\left(\Delta_{*} | \mathbf{x}_{*}, \mathbf{w}, \sigma_{n}^{2}\right)=\mathcal{N}\left(\Delta_{*} |  \varphi\left(\mathbf{x}_{*}\right)^{\mathrm{T}}\mathbf{w}, \sigma_{n}^{2}\right).
\end{equation}
Comparing these to the general result given in Equation \ref{Eq:A-marginal-Gaussian-distribution} and noting that $\varphi(\mathbf{x}_{*})^{\top} \mathbf{A}^{-1}  \varphi(\mathbf{x}_{*})=\varphi(\mathbf{x}_{*}) \mathbf{A}^{-1} \varphi(\mathbf{x}_{*})^{\top}$ gives the predictive mean $\mu_{\Delta_{*}}$ and covariance $\sigma_{\Delta_{*}}$
\begin{equation}
    \mu_{\Delta_{*}}=\frac{1}{\sigma_{n}^{2}}\varphi(\mathbf{x}_{*})^{\top}\mathbf{A}^{-1}\Phi\boldgreek{\Delta}
\end{equation}
\begin{equation}
    \sigma_{\Delta_{*}}^{2}=\sigma_{n}^{2}+\varphi(\mathbf{x}_{*})^{\top} \mathbf{A}^{-1} \varphi(\mathbf{x}_{*}).
\end{equation}
